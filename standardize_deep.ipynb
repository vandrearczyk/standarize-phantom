{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pdb\n",
    "import os, glob\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import vgg19\n",
    "from keras.applications import resnet50\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Activation, core\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.engine import Layer\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics, decomposition, mixture\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "from importlib import reload\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "%matplotlib inline\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_gradient(X, hp_lambda):# where hp_lambda is the constant which multiplies the flipped gradient.\n",
    "    '''Flips the sign of the incoming gradient during training.'''\n",
    "    try:\n",
    "        reverse_gradient.num_calls += 1\n",
    "    except AttributeError:\n",
    "        reverse_gradient.num_calls = 1\n",
    "\n",
    "    grad_name = \"GradientReversal%d\" % reverse_gradient.num_calls\n",
    "\n",
    "    @tf.RegisterGradient(grad_name)\n",
    "    def _flip_gradients(op, grad):\n",
    "        return [tf.negative(grad) * hp_lambda]\n",
    "\n",
    "    g = K.get_session().graph\n",
    "    with g.gradient_override_map({'Identity': grad_name}):\n",
    "        y = tf.identity(X)\n",
    "\n",
    "    return y\n",
    "\n",
    "class GradientReversal(Layer):\n",
    "    '''Flip the sign of gradient during training.'''\n",
    "    def __init__(self, hp_lambda, **kwargs):\n",
    "        super(GradientReversal, self).__init__(**kwargs)\n",
    "        self.supports_masking = False\n",
    "        \n",
    "        #self.hp_lambda = hp_lambda\n",
    "        self.hp_lambda = K.variable(hp_lambda,name='hp_lambda')\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.trainable_weights = []\n",
    "        #super(GradientReversal, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return reverse_gradient(x, self.hp_lambda)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"name\": self.__class__.__name__,\n",
    "                  'hp_lambda': K.get_value(self.hp_lambda)}\n",
    "        base_config = super(GradientReversal, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "def icc(Y, icc_type='icc2'):\n",
    "    ''' Calculate intraclass correlation coefficient for data within\n",
    "        Brain_Data class\n",
    "\n",
    "    ICC Formulas are based on:\n",
    "    Shrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: uses in\n",
    "    assessing rater reliability. Psychological bulletin, 86(2), 420.\n",
    "\n",
    "    icc1:  x_ij = mu + beta_j + w_ij\n",
    "    icc2/3:  x_ij = mu + alpha_i + beta_j + (ab)_ij + epsilon_ij\n",
    "\n",
    "    Code modifed from nipype algorithms.icc\n",
    "    https://github.com/nipy/nipype/blob/master/nipype/algorithms/icc.py\n",
    "\n",
    "    Args:\n",
    "        icc_type: type of icc to calculate (icc: voxel random effect,\n",
    "                icc2: voxel and column random effect, icc3: voxel and\n",
    "                column fixed effect)\n",
    "\n",
    "    Returns:\n",
    "        ICC: intraclass correlation coefficient\n",
    "\n",
    "    '''\n",
    "    #Y = self.data.T\n",
    "    [n, k] = Y.shape\n",
    "\n",
    "    # Degrees of Freedom\n",
    "    dfc = k - 1\n",
    "    dfe = (n - 1) * (k-1)\n",
    "    dfr = n - 1\n",
    "\n",
    "    # Sum Square Total\n",
    "    mean_Y = np.mean(Y)\n",
    "    SST = ((Y - mean_Y) ** 2).sum()\n",
    "\n",
    "    # create the design matrix for the different levels\n",
    "    x = np.kron(np.eye(k), np.ones((n, 1)))  # sessions\n",
    "    x0 = np.tile(np.eye(n), (k, 1))  # subjects\n",
    "    X = np.hstack([x, x0])\n",
    "\n",
    "    # Sum Square Error\n",
    "    predicted_Y = np.dot(np.dot(np.dot(X, np.linalg.pinv(np.dot(X.T, X))),\n",
    "                         X.T), Y.flatten('F'))\n",
    "    residuals = Y.flatten('F') - predicted_Y\n",
    "    SSE = (residuals ** 2).sum()\n",
    "\n",
    "    MSE = SSE / dfe\n",
    "\n",
    "    # Sum square column effect - between colums\n",
    "    SSC = ((np.mean(Y, 0) - mean_Y) ** 2).sum() * n\n",
    "    MSC = SSC / dfc / n\n",
    "\n",
    "    # Sum Square subject effect - between rows/subjects\n",
    "    SSR = SST - SSC - SSE\n",
    "    MSR = SSR / dfr\n",
    "\n",
    "    if icc_type == 'icc1':\n",
    "        # ICC(2,1) = (mean square subject - mean square error) /\n",
    "        # (mean square subject + (k-1)*mean square error +\n",
    "        # k*(mean square columns - mean square error)/n)\n",
    "        # ICC = (MSR - MSRW) / (MSR + (k-1) * MSRW)\n",
    "        NotImplementedError(\"This method isn't implemented yet.\")\n",
    "\n",
    "    elif icc_type == 'icc2':\n",
    "        # ICC(2,1) = (mean square subject - mean square error) /\n",
    "        # (mean square subject + (k-1)*mean square error +\n",
    "        # k*(mean square columns - mean square error)/n)\n",
    "        ICC = (MSR - MSE) / (MSR + (k-1) * MSE + k * (MSC - MSE) / n)\n",
    "\n",
    "    elif icc_type == 'icc3':\n",
    "        # ICC(3,1) = (mean square subject - mean square error) /\n",
    "        # (mean square subject + (k-1)*mean square error)\n",
    "        ICC = (MSR - MSE) / (MSR + (k-1) * MSE)\n",
    "        \n",
    "    return ICC\n",
    "\n",
    "def cartridgeAverage(features,scans,labels):\n",
    "    nb_scans = len(np.unique(scans))\n",
    "    nb_labels = len(np.unique(labels))\n",
    "    dim_features = features.shape[1]\n",
    "    # scans and labels should be 0,0,0...1,1,1,1,...,2,2,.... \n",
    "    for i in range(len(np.unique(scans))):\n",
    "        scans[scans==np.unique(scans)[i]]=i\n",
    "    for i in range(len(np.unique(labels))):\n",
    "        labels[labels==np.unique(labels)[i]]=i\n",
    "    # Average the features within each cartridge.\n",
    "    average_cart_features = np.zeros((nb_scans,nb_labels,dim_features))\n",
    "    nb_slices_per_cart = np.zeros((nb_scans,nb_labels,dim_features))\n",
    "    for i in range(features.shape[0]):\n",
    "        average_cart_features[scans[i],labels[i]]+=features[i]\n",
    "        nb_slices_per_cart[scans[i],labels[i]]+=1\n",
    "    average_cart_features /= nb_slices_per_cart\n",
    "    \n",
    "    return average_cart_features\n",
    "\n",
    "def clusterFeatures(features,labels, **options):\n",
    "    nbClusters = len(np.unique(labels))\n",
    "    # Cluster\n",
    "    kmeans = KMeans(n_clusters=nbClusters, random_state=0).fit(features)\n",
    "    results = metrics.homogeneity_completeness_v_measure(labels, kmeans.labels_)\n",
    "    print('cluster KMEANS:',results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def clusterGMMFeatures(features,labels, **options):\n",
    "    nbClusters = len(np.unique(labels))\n",
    "    # Cluster (covariance type: ['spherical', 'tied', 'diag', 'full'])\n",
    "    gmm = mixture.GaussianMixture(n_components=nbClusters, covariance_type='spherical').fit(features)\n",
    "    \n",
    "    GMMresults = metrics.homogeneity_completeness_v_measure(labels, gmm.predict(features))\n",
    "    print('cluster GMM:',GMMresults)\n",
    "    print('covariances:',gmm.covariances_)\n",
    "    \n",
    "    return [GMMresults, gmm.covariances_]\n",
    "\n",
    "def correlateSize(features,scans,labels):\n",
    "    nScans = len(np.unique(scans))\n",
    "    nLabels = len(np.unique(labels))\n",
    "    dim_features = features.shape[1]\n",
    "    features = np.reshape(features,(nScans,nLabels,dim_features))\n",
    "    size = np.asarray([164,114,102,82,82,82,108,82,82,77,77,82,148,153,128,128,82])\n",
    "    size = size[np.unique(scans)]\n",
    "    pearsons=[]\n",
    "    # scans and labels should be 0,0,0...1,1,1,1,...,2,2,.... \n",
    "    for i in range(len(np.unique(scans))):\n",
    "        scans[scans==np.unique(scans)[i]]=i\n",
    "    for i in range(len(np.unique(labels))):\n",
    "        labels[labels==np.unique(labels)[i]]=i\n",
    "    # Then calculate the correlation with the size of the image\n",
    "    nbnan=0\n",
    "    for iFeat in range(dim_features):\n",
    "        for iLabels in range(nLabels):\n",
    "            feat = np.asarray(features[:,iLabels,iFeat])\n",
    "            pearson = np.corrcoef(feat,size)[0,1]\n",
    "            if ~np.isnan(pearson):\n",
    "                pearsons.append(pearson)\n",
    "            else:\n",
    "                nbnan+=1\n",
    "    pearsons=np.asarray(pearsons)\n",
    "    print('pearson average:',np.average(np.abs(pearsons)),'(nb nan:',nbnan,')')\n",
    "    \n",
    "    return np.average(np.abs(pearsons))\n",
    "\n",
    "def iccAverage(features,nLabels,nScans,type_icc='icc2'):\n",
    "    features = np.reshape(features,(nScans,nLabels,features.shape[1]))\n",
    "    features = features.swapaxes(0,1)\n",
    "    iccs = []\n",
    "    nbnan=0\n",
    "    for i in range(features.shape[2]):\n",
    "        icc1 = icc(features[:,:,i],type_icc)\n",
    "        if not np.isnan(icc1):\n",
    "            iccs.append(icc1)\n",
    "        else:\n",
    "            nbnan+=1\n",
    "    print('ICC: ',np.average(iccs),'(nan',nbnan,')')\n",
    "    \n",
    "    return np.average(iccs)\n",
    "\n",
    "def normalize(x):\n",
    "    av = np.mean(x,axis=0)\n",
    "    stdv = np.std(x,axis=0)\n",
    "    \n",
    "    return (x-av)/stdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all parameters\n",
    "try:\n",
    "    pathSlices = './data/array_slices.mat'\n",
    "    pathLabels = './data/array_labels.mat'\n",
    "    pathScan = './data/array_scan.mat'\n",
    "except:\n",
    "    print('mat files to place in ./data/')\n",
    "# min and max slices values for normalizing\n",
    "minAllSlices = -1409\n",
    "maxAllSlices = 747\n",
    "# Total number classes and scans\n",
    "nbClass = 10\n",
    "nbScans = 17\n",
    "# Number of training classes and scans\n",
    "nTrainLabels = 5\n",
    "nTrainScans = 8 # 17 or 8 in the paper\n",
    "type_icc = 'icc2'\n",
    "# Different flags\n",
    "is_domainAdv=False\n",
    "is_testTrain=True # to evaluate on the train data\n",
    "is_addLayer = True # To add a layer before the prediction layer\n",
    "is_scratch = False # To train from scratch (otherwise load imagenet weights)\n",
    "is_freeze = True # To freeze all but final layers (not to use if training from scratch)\n",
    "is_tsne = False # To plot tsne \n",
    "is_pcaVis = False\n",
    "is_dataAugm=False # Not finished implement\n",
    "\n",
    "# Network to use\n",
    "network = 'VGG' # VGG resnet50\n",
    "# input size of the network\n",
    "inSize = 224\n",
    "# lambda value in the gradient reversal for domain adversarial (1: adversarial. 0: No influence on the main task. -1: The entire network learns the domain)\n",
    "lbd = 1\n",
    "# Batch size\n",
    "batch_size = 32\n",
    "# Learning rate\n",
    "l_r=1e-4\n",
    "# Number of PCA components\n",
    "nPCA = 4\n",
    "# Number of epochs\n",
    "totalEpochs = 2#100\n",
    "# Number of repetitions\n",
    "nbRun = 1#100\n",
    "# For CAM, not used now\n",
    "target_class = 0\n",
    "if is_scratch:\n",
    "    is_freeze = False\n",
    "    \n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_slices = sio.loadmat(pathSlices)['array_slices'][0]\n",
    "array_labels = sio.loadmat(pathLabels)['array_labels'][0]\n",
    "array_scans = sio.loadmat(pathScan)['array_scan'][0]\n",
    "\n",
    "x_all=list()\n",
    "# Resize and preprocess images:\n",
    "for x in array_slices:\n",
    "    x = cv2.resize(x, (inSize, inSize),interpolation = cv2.INTER_LINEAR) \n",
    "    x = 255*(x-minAllSlices)/(maxAllSlices-minAllSlices)\n",
    "    x = np.asarray(np.dstack((x, x, x)), dtype=np.float64)\n",
    "    if network=='VGG':\n",
    "        x_all.append(vgg19.preprocess_input(x))\n",
    "    elif network=='resnet50':\n",
    "        x_all.append(resnet50.preprocess_input(x))\n",
    "x_all=np.asarray(x_all)\n",
    "y_all=array_labels\n",
    "\n",
    "# If we train on all scan/all labels, we also test on all scan/all labels as there is no test left.\n",
    "if nTrainLabels==10:\n",
    "    nTestLabels = nbClass\n",
    "else:\n",
    "    nTestLabels = nbClass-nTrainLabels\n",
    "if nTrainScans==17:\n",
    "    nTestScans = nbScans\n",
    "else:\n",
    "    nTestScans = nbScans-nTrainScans\n",
    "\n",
    "dicResults = {}\n",
    "dicResults['icc_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['icc_pca_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['kmeans_homogeneity_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['kmeans_homogeneity_pca_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['kmeans_completeness_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['kmeans_completeness_pca_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['kmeans_v_measure_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['kmeans_v_measure_pca_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['GMM_homogeneity_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['GMM_homogeneity_pca_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['GMM_completeness_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['GMM_completeness_pca_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['GMM_v_measure_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['GMM_v_measure_pca_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['corrSize_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['corrSize_pca_fc1'] = np.zeros((nbRun,totalEpochs+1))\n",
    "dicResults['acc'] = np.zeros((nbRun,totalEpochs+1))\n",
    "if not is_addLayer:\n",
    "    dicResults['icc_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['icc_pca_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['icc_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['icc_pca_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['kmeans_homogeneity_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['kmeans_homogeneity_pca_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['kmeans_completeness_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['kmeans_completeness_pca_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['kmeans_v_measure_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['kmeans_v_measure_pca_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['GMM_homogeneity_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['GMM_homogeneity_pca_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['GMM_completeness_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['GMM_completeness_pca_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['GMM_v_measure_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['GMM_v_measure_pca_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['corrSize_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    dicResults['corrSize_pca_fc2'] = np.zeros((nbRun,totalEpochs+1))\n",
    "if is_domainAdv:\n",
    "    dicResults['acc_domain'] = np.zeros((nbRun,totalEpochs+1))\n",
    "    \n",
    "clusterResults = np.zeros((nbRun,2,2,3)) # 2 for fc1 and fc2, 2 for with and without PCA, 3 for clustering homogeneity, completeness and v_measure\n",
    "clusterResultsFinetune = np.zeros((nbRun,totalEpochs,2,2,3))\n",
    "clusterGMMResults = np.zeros((nbRun,2,2,3))\n",
    "clusterGMMResultsFinetune = np.zeros((nbRun,totalEpochs,2,2,3))\n",
    "clusterGMMCovResults = np.zeros((nbRun,2,2,nTestLabels))\n",
    "clusterGMMCovResultsFinetune = np.zeros((nbRun,totalEpochs,2,2,nTestLabels))\n",
    "corrResults = np.zeros((nbRun,2,2)) # 2 because with and without pca\n",
    "corrResultsFinetune = np.zeros((nbRun,totalEpochs,2,2))\n",
    "corrFeatResults = np.zeros((nbRun,2,2)) # 2 because with and without pca\n",
    "corrFeatResultsFinetune = np.zeros((nbRun,totalEpochs,2,2))\n",
    "iccResults = np.zeros((nbRun,2,2)) # 2 because with and without pca\n",
    "iccResultsFinetune = np.zeros((nbRun,totalEpochs,2,2))\n",
    "accuracyFinetune = np.zeros((nbRun,totalEpochs))\n",
    "accuracyFinetune2 = np.zeros((nbRun,totalEpochs))\n",
    "\n",
    "for iRun in range(nbRun):        \n",
    "    x_train=list()\n",
    "    y_train=list()\n",
    "\n",
    "    # Free the memory and redefine the model to not run out of memory\n",
    "    K.clear_session()\n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "\n",
    "    if network=='VGG':\n",
    "        if is_scratch:\n",
    "            base_model = vgg19.VGG19(weights=None, include_top=True)\n",
    "        else:\n",
    "            base_model = vgg19.VGG19(weights='imagenet', include_top=True)\n",
    "        if is_addLayer:\n",
    "            x = Dense(100, activation='relu', name='fc3')(base_model.layers[-2].output)\n",
    "            x = Dropout(0.5)(x)\n",
    "            if is_domainAdv:\n",
    "                gr = GradientReversal(hp_lambda=lbd)\n",
    "                x2 = gr(x)\n",
    "                x2 = Dense(100, activation='relu', name='fc3b')(x2)\n",
    "                x2 = Dropout(0.5)(x2)\n",
    "                x2 = Dense(nTrainScans, activation='softmax', name='predictions2')(x2)\n",
    "            x = Dense(nTrainLabels, activation='softmax', name='predictions')(x)\n",
    "        else:\n",
    "            x = Dense(nTrainLabels, activation='softmax', name='predictions')(base_model.layers[-2].output)\n",
    "            if is_domainAdv:\n",
    "                # 2 losses\n",
    "                gr = GradientReversal(hp_lambda=1)\n",
    "                x2 = gr(base_model.layers[-2].output)\n",
    "                x2 = Dense(nTrainScans, activation='softmax', name='predictions2')(x2)\n",
    "\n",
    "    elif network=='resnet50':\n",
    "        if is_scratch:\n",
    "            base_model = resnet50.ResNet50(weights=None, include_top=True)\n",
    "        else:\n",
    "            base_model = resnet50.ResNet50(weights='imagenet', include_top=True)\n",
    "        if is_addLayer:\n",
    "            x = Dense(100, activation='relu', name='fc1')(base_model.layers[-2].output)\n",
    "            x = Dropout(0.5)(x)\n",
    "            if is_domainAdv:\n",
    "                gr = GradientReversal(hp_lambda=lbd)\n",
    "                x2 = gr(x)\n",
    "                x2 = Dense(100, activation='relu', name='fc1c')(x2)\n",
    "                x2 = Dropout(0.5)(x2)\n",
    "                x2 = Dense(nTrainScans, activation='softmax', name='predictions2')(x2)\n",
    "\n",
    "            x = Dense(nTrainLabels, activation='softmax', name='predictions')(x)\n",
    "        else:\n",
    "            x = Dense(nTrainLabels, activation='softmax', name='predictions')(base_model.layers[-2].output)\n",
    "            if is_domainAdv:\n",
    "                # 2 losses\n",
    "                gr = GradientReversal(hp_lambda=1)\n",
    "                x2 = gr(base_model.layers[-2].output)\n",
    "                x2 = Dense(nTrainScans, activation='softmax', name='predictions2')(x2)\n",
    "    #Then create the corresponding model \n",
    "    if is_domainAdv:\n",
    "        model = Model(input=base_model.input, outputs=[x,x2])\n",
    "        if network=='VGG':\n",
    "            list_nofreeze=['fc3','fc3b','predictions','predictions2']\n",
    "        elif network=='resnet50':\n",
    "            list_nofreeze=['fc1','fc1b','predictions','predictions2']\n",
    "    else:\n",
    "        model = Model(input=base_model.input, output=x)   \n",
    "        if network=='VGG':\n",
    "            list_nofreeze=['fc3','predictions']\n",
    "        elif network=='resnet50':\n",
    "            list_nofreeze=['fc1','predictions']\n",
    "\n",
    "    if is_freeze:\n",
    "        for layer in model.layers:\n",
    "            if layer.name not in list_nofreeze:\n",
    "                layer.trainable=False\n",
    "\n",
    "    # Randomly keep some labels and scans for training (test clustering on the rest)\n",
    "    listLabels = list(range(nbClass))\n",
    "    listScans = list(range(nbScans))\n",
    "    np.random.seed(iRun) \n",
    "    np.random.shuffle(listLabels)\n",
    "    trainLabels=sorted(listLabels[0:nTrainLabels])\n",
    "    np.random.shuffle(listScans)\n",
    "    trainScans=sorted(listScans[0:nTrainScans])\n",
    "    print('iRun:',iRun,'****************** training labels:',trainLabels,'scans',trainScans,'********************')\n",
    "    # Create the train set:\n",
    "    idxTrainLabels = np.zeros(y_all.shape, dtype=bool)\n",
    "    idxTrainScans = np.zeros(y_all.shape, dtype=bool)\n",
    "    for n in trainLabels:\n",
    "        idxTrainLabels |= (y_all==n)\n",
    "    for n in trainScans:\n",
    "        idxTrainScans |= (array_scans==n)\n",
    "    idxTrain = idxTrainLabels & idxTrainScans\n",
    "    x_train = x_all[idxTrain]\n",
    "    y_train = y_all[idxTrain]\n",
    "    # Create the test set:\n",
    "    if is_testTrain:\n",
    "        idxTest=idxTrain\n",
    "        nTestLabels=nTrainLabels\n",
    "        nTestScans=nTrainScans\n",
    "    else:\n",
    "        idxTest = np.ones(y_all.shape, dtype=bool)\n",
    "        if not nTrainLabels==10:\n",
    "            for n in trainLabels:\n",
    "                idxTest &= (y_all!=n)\n",
    "        if not nTrainScans==17:\n",
    "            for n in trainScans:\n",
    "                idxTest &= (array_scans!=n)                \n",
    "\n",
    "    # Minimize the number of output labels (0001112222... instead of 0003334444...):\n",
    "    for i in range(len(np.unique(y_train))):\n",
    "        y_train[y_train==np.unique(y_train)[i]]=i\n",
    "\n",
    "    # Convert labels to categorical one-hot encoding\n",
    "    y_train = to_categorical(y_train, num_classes=nTrainLabels)\n",
    "    if is_domainAdv:\n",
    "        y_trainDomain = to_categorical(array_scans[idxTrain], num_classes=nTrainScans)\n",
    "\n",
    "    # Compile model             \n",
    "    #decay_rate = .1 / totalEpochs\n",
    "    decay_rate = 0.\n",
    "    adam = optimizers.Adam(lr=l_r, beta_1=0.9, beta_2=0.999, epsilon=1e-08,decay=decay_rate)\n",
    "    sgd = optimizers.SGD(lr=l_r, momentum=0.9,decay=decay_rate)\n",
    "    if is_domainAdv:\n",
    "        # Use K.variables for the loss weights to be able to modify them during training\n",
    "        lw1 = K.variable(1.)\n",
    "        lw2 = K.variable(1.)            \n",
    "        model.compile(loss='categorical_crossentropy', optimizer=adam,\n",
    "                  metrics=['accuracy'],loss_weights=[lw1, lw2])\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "        #visualize_class_activation_map(model,(x_all[idxTest])[0],target_class)    \n",
    "    print('Results without finetuning:')\n",
    "    \n",
    "    #Cluster the features before finetuning:    \n",
    "    if network=='VGG':  \n",
    "        if is_addLayer:\n",
    "            intermediate_layer_model = Model(inputs=model.input, outputs=[model.get_layer('fc3').output])\n",
    "            fc1_features = np.asarray(intermediate_layer_model.predict(x_all[idxTest]))\n",
    "        else:\n",
    "            intermediate_layer_model = Model(inputs=model.input, outputs=[model.get_layer('fc1').output,model.get_layer('fc2').output])\n",
    "            [fc1_features,fc2_features] = np.asarray(intermediate_layer_model.predict(x_all[idxTest]))\n",
    "    elif network=='resnet50':\n",
    "        if is_addLayer:\n",
    "            intermediate_layer_model = Model(inputs=model.input, outputs=[model.get_layer('fc1').output])            \n",
    "        else:\n",
    "            intermediate_layer_model = Model(inputs=model.input, outputs=[model.get_layer('flatten_1').output])\n",
    "        fc1_features = np.asarray(intermediate_layer_model.predict(x_all[idxTest]))  \n",
    "        fc1_featuresTrain = np.asarray(intermediate_layer_model.predict(x_all[idxTrain]))\n",
    "\n",
    "    # normalize \n",
    "    fc1_features_noAv = normalize(fc1_features)\n",
    "    \n",
    "    # Remove NaN\n",
    "    fc1_features_noAv = fc1_features_noAv[:,~np.isnan(fc1_features_noAv).any(axis=0)]\n",
    "    print('fc1_features_noAv.shape',fc1_features_noAv.shape)\n",
    "    # PCA before averaging\n",
    "    fc1_featuresPCA_noAv = decomposition.PCA(n_components = nPCA).fit_transform(fc1_features_noAv)\n",
    "\n",
    "    # Average features within cartridge:\n",
    "    fc1_features = cartridgeAverage(fc1_features,array_scans[idxTest],y_all[idxTest])\n",
    "    if 'fc2_features' in locals():\n",
    "        fc2_features = cartridgeAverage(fc2_features,array_scans[idxTest],y_all[idxTest])\n",
    "    shapeFeat = fc1_features.shape\n",
    "    labelsAv = np.zeros(shapeFeat[0]*shapeFeat[1],dtype=np.int)\n",
    "    scansAv = np.zeros(shapeFeat[0]*shapeFeat[1],dtype=np.int)\n",
    "    i=0\n",
    "    for i0 in range(shapeFeat[0]):\n",
    "        for i1 in range(shapeFeat[1]):\n",
    "            labelsAv[i]=i1\n",
    "            scansAv[i]=i0\n",
    "            i+=1\n",
    "    fc1_features = np.reshape(fc1_features,(shapeFeat[0]*shapeFeat[1],shapeFeat[2]))\n",
    "    if 'fc2_features' in locals():\n",
    "        fc2_features = np.reshape(fc2_features,(shapeFeat[0]*shapeFeat[1],shapeFeat[2]))\n",
    "\n",
    "    # Normalize \n",
    "    fc1_features = normalize(fc1_features)\n",
    "\n",
    "    if 'fc2_features' in locals():\n",
    "        av = np.mean(fc2_features,axis=0)\n",
    "        stdv = np.std(fc2_features,axis=0)\n",
    "        fc2_features = (fc2_features-av)/stdv\n",
    "\n",
    "    # Remove NaN\n",
    "    fc1_features = fc1_features[:,~np.isnan(fc1_features).any(axis=0)]\n",
    "    if 'fc2_features' in locals():\n",
    "        fc2_features = fc2_features[:,~np.isnan(fc2_features).any(axis=0)]\n",
    "\n",
    "    fc1_featuresPCA = decomposition.PCA(n_components = nPCA).fit_transform(fc1_features)\n",
    "\n",
    "    if 'fc2_features' in locals():\n",
    "        fc2_featuresPCA = decomposition.PCA(n_components = nPCA).fit_transform(fc2_features) \n",
    "\n",
    "    #Intraclass Correlation Coefficient (ICC)\n",
    "    dicResults['icc_fc1'][iRun,0] = iccAverage(fc1_features,nTestLabels,nTestScans,type_icc)\n",
    "    dicResults['icc_pca_fc1'][iRun,0] = iccAverage(fc1_featuresPCA,nTestLabels,nTestScans,type_icc)\n",
    "    if 'fc2_features' in locals():\n",
    "        dicResults['icc_fc2'][iRun,0] = iccAverage(fc2_features,nTestLabels,nTestScans,type_icc)\n",
    "        dicResults['icc_pca_fc2'][iRun,0] = iccAverage(fc2_featuresPCA,nTestLabels,nTestScans,type_icc) \n",
    "\n",
    "    # KMEANS clustering\n",
    "    clusterResults = clusterFeatures(fc1_features,labelsAv, norm = \"normal\")\n",
    "    dicResults['kmeans_homogeneity_fc1'][iRun,0] = clusterResults[0]\n",
    "    dicResults['kmeans_completeness_fc1'][iRun,0] = clusterResults[1]\n",
    "    dicResults['kmeans_v_measure_fc1'][iRun,0] = clusterResults[2]\n",
    "    clusterResultsPCA = clusterFeatures(fc1_featuresPCA,labelsAv, norm = \"normal\")\n",
    "    dicResults['kmeans_homogeneity_pca_fc1'][iRun,0] = clusterResultsPCA[0]\n",
    "    dicResults['kmeans_completeness_pca_fc1'][iRun,0] = clusterResultsPCA[1]\n",
    "    dicResults['kmeans_v_measure_pca_fc1'][iRun,0] = clusterResultsPCA[2]\n",
    "    if 'fc2_features' in locals():\n",
    "        clusterResults = clusterFeatures(fc2_features,labelsAv, norm = \"normal\")\n",
    "        dicResults['kmeans_homogeneity_fc2'][iRun,0] = clusterResults[0]\n",
    "        dicResults['kmeans_completeness_fc2'][iRun,0] = clusterResults[1]\n",
    "        dicResults['kmeans_v_measure_fc2'][iRun,0] = clusterResults[2]\n",
    "        clusterResultsPCA = clusterFeatures(fc2_featuresPCA,labelsAv, norm = \"normal\")\n",
    "        dicResults['kmeans_homogeneity_pca_fc2'][iRun,0] = clusterResultsPCA[0]\n",
    "        dicResults['kmeans_completeness_pca_fc2'][iRun,0] = clusterResultsPCA[1]\n",
    "        dicResults['kmeans_v_measure_pca_fc2'][iRun,0] = clusterResultsPCA[2]\n",
    "\n",
    "    # GMM clustering\n",
    "    clusterResults,_ = clusterGMMFeatures(fc1_features,labelsAv, norm = \"normal\")\n",
    "    dicResults['GMM_homogeneity_fc1'][iRun,0] = clusterResults[0]\n",
    "    dicResults['GMM_completeness_fc1'][iRun,0] = clusterResults[1]\n",
    "    dicResults['GMM_v_measure_fc1'][iRun,0] = clusterResults[2]\n",
    "    clusterResultsPCA,_ = clusterGMMFeatures(fc1_featuresPCA,labelsAv, norm = \"normal\")\n",
    "    dicResults['GMM_homogeneity_pca_fc1'][iRun,0] = clusterResultsPCA[0]\n",
    "    dicResults['GMM_completeness_pca_fc1'][iRun,0] = clusterResultsPCA[1]\n",
    "    dicResults['GMM_v_measure_pca_fc1'][iRun,0] = clusterResultsPCA[2]\n",
    "    if 'fc2_features' in locals():\n",
    "        clusterResults,_ = clusterGMMFeatures(fc2_features,labelsAv, norm = \"normal\")\n",
    "        dicResults['GMM_homogeneity_fc2'][iRun,0] = clusterResults[0]\n",
    "        dicResults['GMM_completeness_fc2'][iRun,0] = clusterResults[1]\n",
    "        dicResults['GMM_v_measure_fc2'][iRun,0] = clusterResults[2]\n",
    "        clusterResultsPCA,_ = clusterGMMFeatures(fc2_featuresPCA,labelsAv, norm = \"normal\")\n",
    "        dicResults['GMM_homogeneity_pca_fc2'][iRun,0] = clusterResultsPCA[0]\n",
    "        dicResults['GMM_completeness_pca_fc2'][iRun,0] = clusterResultsPCA[1]\n",
    "        dicResults['GMM_v_measure_pca_fc2'][iRun,0] = clusterResultsPCA[2]\n",
    "\n",
    "    # Correlation with size\n",
    "    dicResults['corrSize_fc1'][iRun,0] = correlateSize(fc1_features,scansAv,labelsAv)\n",
    "    dicResults['corrSize_pca_fc1'][iRun,0] = correlateSize(fc1_featuresPCA,scansAv,labelsAv)\n",
    "    if 'fc2_features' in locals():\n",
    "        dicResults['corrSize_fc2'][iRun,0] = correlateSize(fc2_features,scansAv,labelsAv)\n",
    "        dicResults['corrSize_pca_fc2'][iRun,0] = correlateSize(fc2_featuresPCA,scansAv,labelsAv)\n",
    "    \n",
    "    if is_tsne:\n",
    "        fc1_features0=fc1_features\n",
    "        fc1_features_noAv0=fc1_features_noAv\n",
    "\n",
    "    if is_pcaVis:\n",
    "        # Copy Pre-finetuning features\n",
    "        fc1_featuresPCA0=fc1_featuresPCA\n",
    "        fc1_featuresPCA_noAv0=fc1_featuresPCA_noAv\n",
    "        \n",
    "    # Accuracy before training:\n",
    "    #print(model.metrics_names)\n",
    "    if is_domainAdv:\n",
    "        dicResults['acc'][iRun,0] = model.evaluate([x_train], [y_train, y_trainDomain], batch_size=batch_size)[3]\n",
    "        dicResults['acc_domain'][iRun,0] = model.evaluate([x_train], [y_train, y_trainDomain], batch_size=batch_size)[4]\n",
    "    else:\n",
    "        dicResults['acc'][iRun,0] = model.evaluate([x_train], y_train, batch_size=batch_size)[1]\n",
    "\n",
    "    # To change the loss weight values\n",
    "    #K.set_value(lw1,1.)\n",
    "    #K.set_value(lw2,0.)\n",
    "\n",
    "    for iEp in range(totalEpochs):\n",
    "        print('iEp:',iEp)\n",
    "        if is_domainAdv:\n",
    "            #if iEp>10:\n",
    "            #    K.set_value(lw1,0)\n",
    "            #    K.set_value(lw2,1)              \n",
    "            #K.set_value(gr.hp_lambda,float(iEp)/totalEpochs)\n",
    "\n",
    "            if is_dataAugm:\n",
    "                reload(image)\n",
    "                datagen = image.ImageDataGenerator(\n",
    "                    #random_resolution=True,\n",
    "                    width_shift_range=.1,\n",
    "                    height_shift_range=.1,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True\n",
    "                    )\n",
    "                datagen.fit(x_train)\n",
    "                hist = model.fit_generator(datagen.flow(x_train, [y_train, y_trainDomain], batch_size=batch_size,\n",
    "                                    save_to_dir='results/augmented_images', \n",
    "                                    save_prefix='augm_im'),\n",
    "                                    steps_per_epoch=len(x_train) / batch_size, epochs=1)\n",
    "            else:\n",
    "                hist = model.fit([x_train], [y_train, y_trainDomain], epochs=1, batch_size=batch_size)\n",
    "        else:\n",
    "            if is_dataAugm:\n",
    "                reload(image)\n",
    "                datagen = image.ImageDataGenerator(\n",
    "                    #random_resolution=True,\n",
    "                    width_shift_range=.1,\n",
    "                    height_shift_range=.1,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True\n",
    "                    )\n",
    "                datagen.fit(x_train)\n",
    "                hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size,\n",
    "                                    save_to_dir='results/augmented_images', \n",
    "                                    save_prefix='augm_im'),\n",
    "                                    steps_per_epoch=len(x_train) / batch_size, epochs=1)\n",
    "            else:\n",
    "                hist = model.fit(x_train, y_train, epochs=1, batch_size=batch_size)\n",
    "\n",
    "\n",
    "        if network=='VGG':\n",
    "            if is_addLayer:\n",
    "                intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer('fc3').output)\n",
    "                fc1_features = np.asarray(intermediate_layer_model.predict(x_all[idxTest]))\n",
    "            else:\n",
    "                intermediate_layer_model = Model(inputs=model.input, outputs=[model.get_layer('fc1').output,model.get_layer('fc2').output])\n",
    "                [fc1_features,fc2_features] = np.asarray(intermediate_layer_model.predict(x_all[idxTest]))\n",
    "        elif network=='resnet50':\n",
    "            if is_addLayer:\n",
    "                intermediate_layer_model = Model(inputs=model.input, outputs=[model.get_layer('fc1').output])            \n",
    "            else:\n",
    "                intermediate_layer_model = Model(inputs=model.input, outputs=[model.get_layer('flatten_1').output])\n",
    "            fc1_features = np.asarray(intermediate_layer_model.predict(x_all[idxTest]))\n",
    "            fc1_featuresTrain = np.asarray(intermediate_layer_model.predict(x_all[idxTrain])) \n",
    "        \n",
    "        # PCA before averaging\n",
    "        # Normalize \n",
    "        fc1_features_noAv = normalize(fc1_features)\n",
    "        \n",
    "        # Remove NaN\n",
    "        fc1_features_noAv = fc1_features_noAv[:,~np.isnan(fc1_features_noAv).any(axis=0)]\n",
    "        fc1_featuresPCA_noAv = decomposition.PCA(n_components = nPCA).fit_transform(fc1_features_noAv)\n",
    "\n",
    "        # Average within cart:\n",
    "        fc1_features = cartridgeAverage(fc1_features,array_scans[idxTest],y_all[idxTest])\n",
    "        if 'fc2_features' in locals():\n",
    "            fc2_features = cartridgeAverage(fc2_features,array_scans[idxTest],y_all[idxTest])\n",
    "\n",
    "        shapeFeat = fc1_features.shape\n",
    "        fc1_features = np.reshape(fc1_features,(shapeFeat[0]*shapeFeat[1],shapeFeat[2]))\n",
    "        if 'fc2_features' in locals():\n",
    "            fc2_features = np.reshape(fc2_features,(shapeFeat[0]*shapeFeat[1],shapeFeat[2]))\n",
    "\n",
    "        # Normalize \n",
    "        fc1_features = normalize(fc1_features)\n",
    "\n",
    "        if 'fc2_features' in locals():\n",
    "            av = np.mean(fc2_features,axis=0)\n",
    "            stdv = np.std(fc2_features,axis=0)\n",
    "            fc2_features = (fc2_features-av)/stdv\n",
    "\n",
    "        # Remove NaN\n",
    "        fc1_features = fc1_features[:,~np.isnan(fc1_features).any(axis=0)]\n",
    "        if 'fc2_features' in locals():\n",
    "            fc2_features = fc2_features[:,~np.isnan(fc2_features).any(axis=0)]\n",
    "\n",
    "        # PCA\n",
    "        fc1_featuresPCA = decomposition.PCA(n_components = nPCA).fit_transform(fc1_features) \n",
    "        if 'fc2_features' in locals():\n",
    "            fc2_featuresPCA = decomposition.PCA(n_components = nPCA).fit_transform(fc2_features) \n",
    "\n",
    "        # Save the accuracy\n",
    "        if is_domainAdv:\n",
    "            dicResults['acc'][iRun,iEp+1] = hist.history['predictions_acc'][0]\n",
    "            dicResults['acc_domain'][iRun,iEp+1] = hist.history['predictions2_acc'][0]\n",
    "        else:\n",
    "            dicResults['acc'][iRun,iEp+1] = hist.history['acc'][0]\n",
    "\n",
    "        #Intraclass Correlation Coefficient (ICC)\n",
    "        dicResults['icc_fc1'][iRun,iEp+1] = iccAverage(fc1_features,nTestLabels,nTestScans,type_icc)\n",
    "        dicResults['icc_pca_fc1'][iRun,iEp+1] = iccAverage(fc1_featuresPCA,nTestLabels,nTestScans,type_icc)\n",
    "        if 'fc2_features' in locals():\n",
    "            dicResults['icc_fc2'][iRun,iEp+1] = iccAverage(fc2_features,nTestLabels,nTestScans,type_icc)\n",
    "            dicResults['icc_pca_fc2'][iRun,iEp+1] = iccAverage(fc2_featuresPCA,nTestLabels,nTestScans,type_icc)\n",
    "\n",
    "        # KMEANS clustering\n",
    "        clusterResults = clusterFeatures(fc1_features,labelsAv, norm = \"normal\")\n",
    "        dicResults['kmeans_homogeneity_fc1'][iRun,iEp+1] = clusterResults[0]\n",
    "        dicResults['kmeans_completeness_fc1'][iRun,iEp+1] = clusterResults[1]\n",
    "        dicResults['kmeans_v_measure_fc1'][iRun,iEp+1] = clusterResults[2]\n",
    "        clusterResultsPCA = clusterFeatures(fc1_featuresPCA,labelsAv, norm = \"normal\")\n",
    "        dicResults['kmeans_homogeneity_pca_fc1'][iRun,iEp+1] = clusterResultsPCA[0]\n",
    "        dicResults['kmeans_completeness_pca_fc1'][iRun,iEp+1] = clusterResultsPCA[1]\n",
    "        dicResults['kmeans_v_measure_pca_fc1'][iRun,iEp+1] = clusterResultsPCA[2]\n",
    "        if 'fc2_features' in locals():\n",
    "            clusterResults = clusterFeatures(fc2_features,labelsAv, norm = \"normal\")\n",
    "            dicResults['kmeans_homogeneity_fc2'][iRun,iEp+1] = clusterResults[0]\n",
    "            dicResults['kmeans_completeness_fc2'][iRun,iEp+1] = clusterResults[1]\n",
    "            dicResults['kmeans_v_measure_fc2'][iRun,iEp+1] = clusterResults[2]\n",
    "            clusterResultsPCA = clusterFeatures(fc2_featuresPCA,labelsAv, norm = \"normal\")\n",
    "            dicResults['kmeans_homogeneity_pca_fc2'][iRun,iEp+1] = clusterResultsPCA[0]\n",
    "            dicResults['kmeans_completeness_pca_fc2'][iRun,iEp+1] = clusterResultsPCA[1]\n",
    "            dicResults['kmeans_v_measure_pca_fc2'][iRun,iEp+1] = clusterResultsPCA[2]\n",
    "            \n",
    "        # GMM clustering\n",
    "        clusterResults,_ = clusterGMMFeatures(fc1_features,labelsAv, norm = \"normal\")\n",
    "        dicResults['GMM_homogeneity_fc1'][iRun,iEp+1] = clusterResults[0]\n",
    "        dicResults['GMM_completeness_fc1'][iRun,iEp+1] = clusterResults[1]\n",
    "        dicResults['GMM_v_measure_fc1'][iRun,iEp+1] = clusterResults[2]\n",
    "        clusterResultsPCA,_ = clusterGMMFeatures(fc1_featuresPCA,labelsAv, norm = \"normal\")\n",
    "        dicResults['GMM_homogeneity_pca_fc1'][iRun,iEp+1] = clusterResultsPCA[0]\n",
    "        dicResults['GMM_completeness_pca_fc1'][iRun,iEp+1] = clusterResultsPCA[1]\n",
    "        dicResults['GMM_v_measure_pca_fc1'][iRun,iEp+1] = clusterResultsPCA[2]\n",
    "        if 'fc2_features' in locals():\n",
    "            clusterResults,_ = clusterGMMFeatures(fc2_features,labelsAv, norm = \"normal\")\n",
    "            dicResults['GMM_homogeneity_fc2'][iRun,iEp+1] = clusterResults[0]\n",
    "            dicResults['GMM_completeness_fc2'][iRun,iEp+1] = clusterResults[1]\n",
    "            dicResults['GMM_v_measure_fc2'][iRun,iEp+1] = clusterResults[2]\n",
    "            clusterResultsPCA,_ = clusterGMMFeatures(fc2_featuresPCA,labelsAv, norm = \"normal\")\n",
    "            dicResults['GMM_homogeneity_pca_fc2'][iRun,iEp+1] = clusterResultsPCA[0]\n",
    "            dicResults['GMM_completeness_pca_fc2'][iRun,iEp+1] = clusterResultsPCA[1]\n",
    "            dicResults['GMM_v_measure_pca_fc2'][iRun,iEp+1] = clusterResultsPCA[2]\n",
    "            \n",
    "        # Correlation with size\n",
    "        dicResults['corrSize_fc1'][iRun,iEp+1] = correlateSize(fc1_features,scansAv,labelsAv)\n",
    "        dicResults['corrSize_pca_fc1'][iRun,iEp+1] = correlateSize(fc1_featuresPCA,scansAv,labelsAv)\n",
    "        if 'fc2_features' in locals():\n",
    "            dicResults['corrSize_fc2'][iRun,iEp+1] = correlateSize(fc2_features,scansAv,labelsAv)\n",
    "            dicResults['corrSize_pca_fc2'][iRun,iEp+1] = correlateSize(fc2_featuresPCA,scansAv,labelsAv)\n",
    "        if iEp%10==0:\n",
    "            if is_tsne:\n",
    "                # t-SNE to visualize entanglement\n",
    "                fig = plt.figure(figsize=(30, 20))\n",
    "                Y = TSNE(n_components=2).fit_transform(fc1_features0)\n",
    "                ax = fig.add_subplot(2, 4, 1)\n",
    "                # t-SNE  of test features (or train when testTrain) averaged within cart with labels colors\n",
    "                plt.scatter(Y[:, 0], Y[:, 1], c=labelsAv, marker='x', linewidths=.5, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=10)\n",
    "\n",
    "                # t-SNE  of test features averaged within cart with scans colors\n",
    "                ax = fig.add_subplot(2, 4, 2)\n",
    "                plt.scatter(Y[:, 0], Y[:, 1], c=scansAv, marker='x', linewidths=.5, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=17)\n",
    "\n",
    "                # _all in the sense before averaging.  t-SNE of test features before averaging with labels colors\n",
    "                Y = TSNE(n_components=2).fit_transform(fc1_features_noAv0)\n",
    "                ax = fig.add_subplot(2, 4, 3)\n",
    "                plt.scatter(Y[:, 0], Y[:, 1], c=y_all[idxTest], marker='x', linewidths=.2, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=10)\n",
    "                # t-SNE of test features before averaging with scans colors\n",
    "                ax = fig.add_subplot(2, 4, 4)\n",
    "                plt.scatter(Y[:, 0], Y[:, 1], c=array_scans[idxTest], marker='x', linewidths=.2, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=17)\n",
    "\n",
    "                Y = TSNE(n_components=2).fit_transform(fc1_features)\n",
    "                ax = fig.add_subplot(2, 4, 5)\n",
    "                plt.scatter(Y[:, 0], Y[:, 1], c=labelsAv, marker='x', linewidths=.5, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=10)\n",
    "\n",
    "                ax = fig.add_subplot(2, 4, 6)\n",
    "                plt.scatter(Y[:, 0], Y[:, 1], c=scansAv, marker='x', linewidths=.5, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=17)\n",
    "\n",
    "                Y = TSNE(n_components=2).fit_transform(fc1_features_noAv)\n",
    "                ax = fig.add_subplot(2, 4, 7)\n",
    "                plt.scatter(Y[:, 0], Y[:, 1], c=y_all[idxTest], marker='x', linewidths=.2, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=10)\n",
    "\n",
    "                ax = fig.add_subplot(2, 4, 8)\n",
    "                plt.scatter(Y[:, 0], Y[:, 1], c=array_scans[idxTest], marker='x', linewidths=.2, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=17)\n",
    "\n",
    "                ax.xaxis.set_major_formatter(NullFormatter())\n",
    "                ax.yaxis.set_major_formatter(NullFormatter())\n",
    "                plt.axis('tight')\n",
    "                # Plots can be saved for each run, each epoch with:\n",
    "                #fig.savefig('results/tsne/tsne_'+str(iRun)+'_'+str(iEp)+'.svg', bbox_inches='tight')\n",
    "            if is_pcaVis:\n",
    "                # PCA first components visualization\n",
    "                fig = plt.figure(figsize=(30, 20))\n",
    "                ax = fig.add_subplot(2, 4, 1)\n",
    "                # PCA of test features (or train when testTrain) averaged within cart with labels colors\n",
    "                plt.scatter(fc1_featuresPCA0[:, 1], fc1_featuresPCA0[:, 0], c=labelsAv, marker='x', linewidths=.5, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=10)\n",
    "                ax = fig.add_subplot(2, 4, 2)\n",
    "                # PCA of test features averaged within cart with scans colors\n",
    "                plt.scatter(fc1_featuresPCA0[:, 1], fc1_featuresPCA0[:, 0], c=scansAv, marker='x', linewidths=.5, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=17)\n",
    "\n",
    "                ax = fig.add_subplot(2, 4, 3)\n",
    "                # _all in the sense before averaging.  PCA of test features before averaging with labels colors\n",
    "                plt.scatter(fc1_featuresPCA_noAv0[:, 1], fc1_featuresPCA_noAv0[:, 0], c=y_all[idxTest], marker='x', linewidths=.2, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=10)\n",
    "                ax = fig.add_subplot(2, 4, 4)\n",
    "                #PCA of test features before averaging with scans colors\n",
    "                plt.scatter(fc1_featuresPCA_noAv0[:, 1], fc1_featuresPCA_noAv0[:, 0], c=array_scans[idxTest], marker='x', linewidths=.2, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=17)\n",
    "\n",
    "                ax = fig.add_subplot(2, 4, 5)\n",
    "                plt.scatter(fc1_featuresPCA[:, 1], fc1_featuresPCA[:, 0], c=labelsAv, marker='x', linewidths=.5, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=10)\n",
    "                ax = fig.add_subplot(2, 4, 6)\n",
    "                plt.scatter(fc1_featuresPCA[:, 1], fc1_featuresPCA[:, 0], c=scansAv, marker='x', linewidths=.5, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=17)\n",
    "\n",
    "                ax = fig.add_subplot(2, 4, 7)\n",
    "                plt.scatter(fc1_featuresPCA_noAv[:, 1], fc1_featuresPCA_noAv[:, 0], c=y_all[idxTest], marker='x', linewidths=.5, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=10)\n",
    "                ax = fig.add_subplot(2, 4, 8)\n",
    "                plt.scatter(fc1_featuresPCA_noAv[:, 1], fc1_featuresPCA_noAv[:, 0], c=array_scans[idxTest], marker='x', linewidths=.5, cmap=plt.cm.get_cmap(\"gist_ncar\"), vmin=0, vmax=17)\n",
    "\n",
    "                ax.xaxis.set_major_formatter(NullFormatter())\n",
    "                ax.yaxis.set_major_formatter(NullFormatter())\n",
    "                plt.axis('tight')\n",
    "                # Plots can be saved for each run, each epoch with:\n",
    "                #fig.savefig('results/pca/pca_'+str(iRun)+'_'+str(iEp)+'.svg', bbox_inches='tight')\n",
    "\n",
    "                \n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "    \n",
    "# Save the dictionary of results as npy (if more stats are needed):\n",
    "np.save('./results/dicResults.npy', dicResults)\n",
    "\n",
    "# Average results across runs\n",
    "dicAvResults = {}\n",
    "for key in dicResults.keys():\n",
    "    dicAvResults[key]= np.mean(dicResults[key],axis=0)\n",
    "    \n",
    "# Save results as csv file\n",
    "csv_path = './results/results.csv'\n",
    "with open(csv_path, 'w') as f:\n",
    "    for key in sorted(dicAvResults.keys()):\n",
    "        f.write(\"%s,%s\\n\"%(key,dicAvResults[key]))\n",
    "\n",
    "print('Results saved in:',csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicAvResults"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
