{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import radiomics\n",
    "import six\n",
    "import SimpleITK as sitk\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics, decomposition, mixture, discriminant_analysis\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Activation, merge, core\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icc(Y, icc_type='icc2'):\n",
    "    ''' Calculate intraclass correlation coefficient for data within\n",
    "        Brain_Data class\n",
    "\n",
    "    ICC Formulas are based on:\n",
    "    Shrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: uses in\n",
    "    assessing rater reliability. Psychological bulletin, 86(2), 420.\n",
    "\n",
    "    icc1:  x_ij = mu + beta_j + w_ij\n",
    "    icc2/3:  x_ij = mu + alpha_i + beta_j + (ab)_ij + epsilon_ij\n",
    "\n",
    "    Code modifed from nipype algorithms.icc\n",
    "    https://github.com/nipy/nipype/blob/master/nipype/algorithms/icc.py\n",
    "\n",
    "    Args:\n",
    "        icc_type: type of icc to calculate (icc: voxel random effect,\n",
    "                icc2: voxel and column random effect, icc3: voxel and\n",
    "                column fixed effect)\n",
    "\n",
    "    Returns:\n",
    "        ICC: intraclass correlation coefficient\n",
    "\n",
    "    '''\n",
    "\n",
    "    #Y = self.data.T\n",
    "    [n, k] = Y.shape\n",
    "    # Degrees of Freedom\n",
    "    dfc = k - 1\n",
    "    dfe = (n - 1) * (k-1)\n",
    "    dfr = n - 1\n",
    "\n",
    "    # Sum Square Total\n",
    "    mean_Y = np.mean(Y)\n",
    "    SST = ((Y - mean_Y) ** 2).sum()\n",
    "\n",
    "    # create the design matrix for the different levels\n",
    "    x = np.kron(np.eye(k), np.ones((n, 1)))  # sessions\n",
    "    x0 = np.tile(np.eye(n), (k, 1))  # subjects\n",
    "    X = np.hstack([x, x0])\n",
    "\n",
    "    # Sum Square Error\n",
    "    predicted_Y = np.dot(np.dot(np.dot(X, np.linalg.pinv(np.dot(X.T, X))),\n",
    "                         X.T), Y.flatten('F'))\n",
    "    residuals = Y.flatten('F') - predicted_Y\n",
    "    SSE = (residuals ** 2).sum()\n",
    "\n",
    "    MSE = SSE / dfe\n",
    "\n",
    "    # Sum square column effect - between colums\n",
    "    SSC = ((np.mean(Y, 0) - mean_Y) ** 2).sum() * n\n",
    "    MSC = SSC / dfc / n\n",
    "\n",
    "    # Sum Square subject effect - between rows/subjects\n",
    "    SSR = SST - SSC - SSE\n",
    "    MSR = SSR / dfr\n",
    "\n",
    "    if icc_type == 'icc1':\n",
    "        # ICC(2,1) = (mean square subject - mean square error) /\n",
    "        # (mean square subject + (k-1)*mean square error +\n",
    "        # k*(mean square columns - mean square error)/n)\n",
    "        # ICC = (MSR - MSRW) / (MSR + (k-1) * MSRW)\n",
    "        NotImplementedError(\"This method isn't implemented yet.\")\n",
    "\n",
    "    elif icc_type == 'icc2':\n",
    "        # ICC(2,1) = (mean square subject - mean square error) /\n",
    "        # (mean square subject + (k-1)*mean square error +\n",
    "        # k*(mean square columns - mean square error)/n)\n",
    "        ICC = (MSR - MSE) / (MSR + (k-1) * MSE + k * (MSC - MSE) / n)\n",
    "\n",
    "    elif icc_type == 'icc3':\n",
    "        # ICC(3,1) = (mean square subject - mean square error) /\n",
    "        # (mean square subject + (k-1)*mean square error)\n",
    "        ICC = (MSR - MSE) / (MSR + (k-1) * MSE)\n",
    "\n",
    "    return ICC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare with the paper (eg. Fig 4), we have the following order of labels:\n",
    "# 0: 50% fill\n",
    "# 1: 40% fill\n",
    "# 2: 30% fill\n",
    "# 3: 20% fill\n",
    "# 4: Sycamore\n",
    "# 5: Rubber particles\n",
    "# 6: Dense Cork\n",
    "# 7: Acrylic\n",
    "# 8: Cork\n",
    "# 9: 3D printed plaster\n",
    "\n",
    "pathSlices = './data/array_slices.mat'\n",
    "pathLabels = './data/array_labels.mat'\n",
    "pathScan = './data/array_scan.mat'\n",
    "array_slices = sio.loadmat(pathSlices)['array_slices'][0]\n",
    "array_labels = sio.loadmat(pathLabels)['array_labels'][0]\n",
    "array_scan = sio.loadmat(pathScan)['array_scan'][0]\n",
    "\n",
    "cartridge_interest = 5 # 5 is rubber cartridge like in the paper\n",
    "nb_class = 17\n",
    "features = np.zeros((len(array_slices),8))\n",
    "featuresPy = []\n",
    "\n",
    "for i in range(len(array_slices)):\n",
    "    print '{0}\\r'.format(i),  \n",
    "    # resize to in-plane pixel spacing of 1mm2 (160x160)\n",
    "    # INTER_LINEAR or INTER_CUBIC INTER_NEAREST INTER_AREA INTER_LANCZOS4 \n",
    "    im = cv2.resize(array_slices[i], (160, 160),interpolation = cv2.INTER_LINEAR) \n",
    "    im = np.asarray(im).astype(np.int16)\n",
    "    im=np.expand_dims(im,axis=0)\n",
    "    \n",
    "    lab = np.ones(np.asarray(im).shape)\n",
    "    lab = np.asarray(lab).astype(np.int32)\n",
    "    img = sitk.GetImageFromArray(im)\n",
    "    label = sitk.GetImageFromArray(lab)\n",
    "        \n",
    "    # All Pyradiomics features:\n",
    "    extractor = radiomics.featureextractor.RadiomicsFeaturesExtractor()\n",
    "    result = extractor.execute(img, label)\n",
    "    f = []\n",
    "    i=0\n",
    "    for key, val in six.iteritems(result):\n",
    "        if \"general\" not in key and \"original_shape\" not in key:\n",
    "            i+=1\n",
    "            f.append(val)\n",
    "    featuresPy.append(np.asarray(f))\n",
    "    \n",
    "    # Only a selection of features:\n",
    "    extractor = radiomics.featureextractor.RadiomicsFeaturesExtractor()\n",
    "    extractor.disableAllFeatures()\n",
    "    extractor.enableFeaturesByName(firstorder=['Minimum','Maximum','Mean','StandardDeviation','Entropy'])\n",
    "    extractor.enableFeaturesByName(ngtdm=['Busyness','Coarseness','Contrast','Complexity','Strength'])\n",
    "    result = extractor.execute(img, label)\n",
    "    features[i,0] = result[\"original_firstorder_StandardDeviation\"]\n",
    "    features[i,1] = result[\"original_firstorder_Mean\"]\n",
    "    features[i,2] = result[\"original_firstorder_Entropy\"]\n",
    "    features[i,3] = result[\"original_ngtdm_Coarseness\"]\n",
    "    features[i,4] = result[\"original_ngtdm_Complexity\"]\n",
    "    features[i,5] = result[\"original_ngtdm_Strength\"]\n",
    "    features[i,6] = result[\"original_ngtdm_Busyness\"]\n",
    "    features[i,7] = result[\"original_ngtdm_Contrast\"]\n",
    "    \n",
    "featuresPy = np.asarray(featuresPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartridgeAverage(features,scans,labels):\n",
    "    nb_scans = len(np.unique(scans))\n",
    "    nb_labels = len(np.unique(labels))\n",
    "    dim_features = features.shape[1]\n",
    "    # scans and labels should be 0,0,0...1,1,1,1,...,2,2,.... \n",
    "    for i in range(len(np.unique(scans))):\n",
    "        scans[scans==np.unique(scans)[i]]=i\n",
    "    for i in range(len(np.unique(labels))):\n",
    "        labels[labels==np.unique(labels)[i]]=i\n",
    "    # First average the features within each cartridge.\n",
    "    average_cart_features = np.zeros((nb_scans,nb_labels,dim_features))\n",
    "    nb_slices_per_cart = np.zeros((nb_scans,nb_labels,dim_features))\n",
    "    for i in range(features.shape[0]):\n",
    "        average_cart_features[scans[i],labels[i]]+=features[i]\n",
    "        nb_slices_per_cart[scans[i],labels[i]]+=1\n",
    "    average_cart_features /= nb_slices_per_cart\n",
    "    return average_cart_features\n",
    "\n",
    "def clusterFeatures(features,labels, **options):#norm=1, riesz=0):\n",
    "    nbClusters = len(np.unique(labels))\n",
    "    # Cluster\n",
    "    kmeans = KMeans(n_clusters=nbClusters, random_state=0).fit(features)\n",
    "    results = metrics.homogeneity_completeness_v_measure(labels, kmeans.labels_)\n",
    "    print 'cluster KMEANS:',results\n",
    "    \n",
    "    return results\n",
    "\n",
    "def clusterGMMFeatures(features,labels, **options):# ,featuresTrain,labelsTrain      \n",
    "    nbClusters = len(np.unique(labels))\n",
    "    gmm = mixture.GaussianMixture(n_components=nbClusters, covariance_type='spherical').fit(features)\n",
    "    \n",
    "    results = metrics.homogeneity_completeness_v_measure(labels, gmm.predict(features))\n",
    "    print 'cluster GMM:',results\n",
    "    print 'covariances:',gmm.covariances_\n",
    "    \n",
    "    return [results, gmm.covariances_]\n",
    "\n",
    "def correlateSize(features,scans,labels):\n",
    "    nScans = len(np.unique(scans))\n",
    "    nLabels = len(np.unique(labels))\n",
    "    dim_features = features.shape[1]\n",
    "    features = np.reshape(features,(nScans,nLabels,dim_features))\n",
    "    size = np.asarray([164,114,102,82,82,82,108,82,82,77,77,82,148,153,128,128,82])\n",
    "    size = size[np.unique(scans)]\n",
    "    pearsons=[]\n",
    "    # scans and labels should be 0,0,0...1,1,1,1,...,2,2,.... \n",
    "    for i in range(len(np.unique(scans))):\n",
    "        scans[scans==np.unique(scans)[i]]=i\n",
    "    for i in range(len(np.unique(labels))):\n",
    "        labels[labels==np.unique(labels)[i]]=i\n",
    "    \n",
    "    # Then calculate the correlation with the size of the image\n",
    "    nbnan=0\n",
    "    for iFeat in range(dim_features):\n",
    "        for iLabels in range(nLabels):\n",
    "            a = np.asarray(features[:,iLabels,iFeat])\n",
    "            b = size\n",
    "            pearson = np.corrcoef(a,b)[0,1]\n",
    "            if ~np.isnan(pearson):\n",
    "                pearsons.append(pearson)\n",
    "            else:\n",
    "                nbnan+=1\n",
    "    pearsons=np.asarray(pearsons)\n",
    "    print 'pearson average:',np.average(np.abs(pearsons)),'(nb nan:',nbnan,')'\n",
    "    \n",
    "    return np.average(np.abs(pearsons))\n",
    "\n",
    "def iccAverage(features,nLabels,nScans,type_icc='icc2'):\n",
    "    features = np.reshape(features,(nScans,nLabels,features.shape[1]))\n",
    "    features = features.swapaxes(0,1)\n",
    "    iccs = []\n",
    "    nbnan=0\n",
    "    for i in range(features.shape[2]):\n",
    "        icc1 = icc(features[:,:,i])\n",
    "        if not np.isnan(icc1):\n",
    "            iccs.append(icc(features[:,:,i],type_icc))\n",
    "        else:\n",
    "            nbnan+=1\n",
    "    print 'ICC: ',np.average(iccs),'(nan',nbnan,')'\n",
    "    \n",
    "    \n",
    "    return np.average(iccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeResults(listResults,path_results):\n",
    "    \n",
    "    iccAvResultsFinetune = listResults[0]\n",
    "    iccAvResults = listResults[1]\n",
    "    clusterResultsFinetune =  listResults[2]\n",
    "    clusterResults = listResults[3]\n",
    "    clusterGMMResultsFinetune = listResults[4]\n",
    "    clusterGMMResults = listResults[5]\n",
    "    clusterGMMCovResultsFinetune = listResults[6]\n",
    "    clusterGMMCovResults = listResults[7]\n",
    "    corrResultsFinetune = listResults[8]\n",
    "    corrResults = listResults[9]\n",
    "    accuracyFinetune = listResults[10]\n",
    "    \n",
    "    iccAvResultsNonzero = []\n",
    "    iccAvResultsFinetuneNonzero = []\n",
    "    clusterResultsNonzero = []\n",
    "    clusterResultsFinetuneNonzero = []\n",
    "    clusterGMMResultsNonzero = []\n",
    "    clusterGMMResultsFinetuneNonzero = []\n",
    "    clusterGMMCovResultsNonzero = []\n",
    "    clusterGMMCovResultsFinetuneNonzero = []\n",
    "    corrResultsNonzero = []\n",
    "    corrResultsFinetuneNonzero = []\n",
    "    accuracyFinetuneNonzero = []\n",
    "    # When trainVGG.ipynb is training, it fills the results and some are still 0. I discard them to calculate the average.\n",
    "    print clusterResults.shape\n",
    "    for i in range(clusterResults.shape[0]):\n",
    "        if clusterResults[i,0,0]!=0:\n",
    "            iccAvResultsNonzero.append(iccAvResults[i])\n",
    "            iccAvResultsFinetuneNonzero.append(iccAvResultsFinetune[i])\n",
    "            clusterResultsNonzero.append(clusterResults[i])\n",
    "            clusterResultsFinetuneNonzero.append(clusterResultsFinetune[i])\n",
    "            clusterGMMResultsNonzero.append(clusterGMMResults[i])\n",
    "            clusterGMMResultsFinetuneNonzero.append(clusterGMMResultsFinetune[i])\n",
    "            clusterGMMCovResultsNonzero.append(clusterGMMCovResults[i])\n",
    "            clusterGMMCovResultsFinetuneNonzero.append(clusterGMMCovResultsFinetune[i])\n",
    "            corrResultsNonzero.append(corrResults[i])\n",
    "            corrResultsFinetuneNonzero.append(corrResultsFinetune[i])\n",
    "            accuracyFinetuneNonzero.append(accuracyFinetune[i])\n",
    "        else:\n",
    "            break\n",
    "    iccAvResults = np.asarray(iccAvResultsNonzero)\n",
    "    iccAvResultsFinetune = np.asarray(iccAvResultsFinetuneNonzero)\n",
    "    clusterResults = np.asarray(clusterResultsNonzero)\n",
    "    clusterResultsFinetune = np.asarray(clusterResultsFinetuneNonzero)\n",
    "    clusterGMMResults = np.asarray(clusterGMMResultsNonzero)\n",
    "    clusterGMMResultsFinetune = np.asarray(clusterGMMResultsFinetuneNonzero)\n",
    "    clusterGMMCovResults = np.asarray(clusterGMMCovResultsNonzero)\n",
    "    clusterGMMCovResultsFinetune = np.asarray(clusterGMMCovResultsFinetuneNonzero)\n",
    "    corrResults = np.asarray(corrResultsNonzero)\n",
    "    corrResultsFinetune = np.asarray(corrResultsFinetuneNonzero)\n",
    "    accuracyFinetune = np.asarray(accuracyFinetuneNonzero)\n",
    "\n",
    "    f = open(path_results+\"results.txt\", \"w\")\n",
    "    f.write(str(iccAvResults.shape)+ str(iccAvResultsFinetune.shape))\n",
    "    \n",
    "    f.write( \"\\n**** Intraclass Correlation Coefficient ICC ****\")\n",
    "    # Without PCA\n",
    "    pca = 0\n",
    "    f.write(\"\\nICC without PCA pyradiomics %.4f(%.5f) \" % (np.mean(iccAvResults[:,pca]), np.std(iccAvResults[:,pca]))+\",\")\n",
    "    for i in range(iccAvResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(iccAvResultsFinetune[:,i,pca]), np.std(iccAvResultsFinetune[:,i,pca])))\n",
    "   \n",
    "    # With PCA\n",
    "    pca = 1\n",
    "    f.write(\"\\nICC with PCA pyradiomics %.4f(%.5f) \" % (np.mean(iccAvResults[:,pca]), np.std(iccAvResults[:,pca]))+\",\")\n",
    "    for i in range(iccAvResultsFinetune.shape[1]):     \n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(iccAvResultsFinetune[:,i,pca]), np.std(iccAvResultsFinetune[:,i,pca])))   \n",
    "   \n",
    "    f.write(\"\\n**** cluster K-MEANS of the same class (larger is better) ****\")\n",
    "    f.write(\"\\nwithout PCA pyradiomics\")\n",
    "    pca = 0\n",
    "    f.write(\"\\n homogeneity %.4f(%.5f) \" % (np.mean(clusterResults[:,pca,0]), np.std(clusterResults[:,pca,0]))+\",\")\n",
    "    for i in range(clusterResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterResultsFinetune[:,i,pca,0]), np.std(clusterResultsFinetune[:,i,pca,0])))\n",
    "    f.write(\"\\n completeness %.4f(%.5f) \" % (np.mean(clusterResults[:,pca,1]), np.std(clusterResults[:,pca,1]))+\",\")\n",
    "    for i in range(clusterResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterResultsFinetune[:,i,pca,1]), np.std(clusterResultsFinetune[:,i,pca,1])))\n",
    "    f.write(\"\\n v_measure %.4f(%.5f) \" % (np.mean(clusterResults[:,pca,2]), np.std(clusterResults[:,pca,2]))+\",\")\n",
    "    for i in range(clusterResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterResultsFinetune[:,i,pca,2]), np.std(clusterResultsFinetune[:,i,pca,2])))\n",
    "\n",
    "    f.write(\"\\nwith PCA pyradiomics\")\n",
    "    pca = 1\n",
    "    f.write(\"\\n homogeneity %.4f(%.5f) \" % (np.mean(clusterResults[:,pca,0]), np.std(clusterResults[:,pca,0]))+\",\")\n",
    "    for i in range(clusterResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterResultsFinetune[:,i,pca,0]), np.std(clusterResultsFinetune[:,i,pca,0])))\n",
    "    f.write(\"\\n complet. %.4f(%.5f) \" % (np.mean(clusterResults[:,pca,1]), np.std(clusterResults[:,pca,1]))+\",\")\n",
    "    for i in range(clusterResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterResultsFinetune[:,i,pca,1]), np.std(clusterResultsFinetune[:,i,pca,1])))\n",
    "    f.write(\"\\n v_meas. %.4f(%.5f) \" % (np.mean(clusterResults[:,pca,2]), np.std(clusterResults[:,pca,2]))+\",\")\n",
    "    for i in range(clusterResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterResultsFinetune[:,i,pca,2]), np.std(clusterResultsFinetune[:,i,pca,2])))\n",
    "\n",
    "    f.write(\"\\n**** cluster GMM of the same class (larger is better) ****\")\n",
    "    f.write(\"\\nwithout PCA pyradiomics\")\n",
    "    pca = 0\n",
    "    f.write(\"\\n homog. %.4f(%.5f) \" % (np.mean(clusterGMMResults[:,pca,0]), np.std(clusterGMMResults[:,pca,0]))+\",\")\n",
    "    for i in range(clusterGMMResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterGMMResultsFinetune[:,i,pca,0]), np.std(clusterGMMResultsFinetune[:,i,pca,0])))\n",
    "    f.write(\"\\n complet. %.4f(%.5f) \" % (np.mean(clusterGMMResults[:,pca,1]), np.std(clusterGMMResults[:,pca,1]))+\",\")\n",
    "    for i in range(clusterGMMResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterGMMResultsFinetune[:,i,pca,1]), np.std(clusterGMMResultsFinetune[:,i,pca,1])))\n",
    "    f.write(\"\\n v_meas. %.4f(%.5f) \" % (np.mean(clusterGMMResults[:,pca,2]), np.std(clusterGMMResults[:,pca,2]))+\",\")\n",
    "    for i in range(clusterGMMResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterGMMResultsFinetune[:,i,pca,2]), np.std(clusterGMMResultsFinetune[:,i,pca,2])))\n",
    "    f.write(\"\\n av. cov. %.4f(%.5f) \" % (np.mean(clusterGMMCovResults[:,pca]), np.std(np.mean(clusterGMMCovResults[:,pca],axis=1)))+\",\")\n",
    "    for i in range(clusterGMMCovResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterGMMCovResultsFinetune[:,i,pca]), np.std(np.mean(clusterGMMCovResultsFinetune[:,i,pca],axis=1))))\n",
    "    \n",
    "    f.write(\"\\nwith PCA pyradiomics\")\n",
    "    pca = 1\n",
    "    f.write(\"\\n homog. %.4f(%.5f) \" % (np.mean(clusterGMMResults[:,pca,0]), np.std(clusterGMMResults[:,pca,0]))+\",\")\n",
    "    for i in range(clusterGMMResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterGMMResultsFinetune[:,i,pca,0]), np.std(clusterGMMResultsFinetune[:,i,pca,0])))\n",
    "    f.write(\"\\n complet. %.4f(%.5f) \" % (np.mean(clusterGMMResults[:,pca,1]), np.std(clusterGMMResults[:,pca,1]))+\",\")\n",
    "    for i in range(clusterGMMResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterGMMResultsFinetune[:,i,pca,1]), np.std(clusterGMMResultsFinetune[:,i,pca,1])))\n",
    "    f.write(\"\\n v_meas. %.4f(%.5f) \" % (np.mean(clusterGMMResults[:,pca,2]), np.std(clusterGMMResults[:,pca,2]))+\",\")\n",
    "    for i in range(clusterGMMResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterGMMResultsFinetune[:,i,pca,2]), np.std(clusterGMMResultsFinetune[:,i,pca,2])))\n",
    "    f.write(\"\\n av. cov. %.4f(%.5f) \" % (np.mean(clusterGMMCovResults[:,pca]), np.std(np.mean(clusterGMMCovResults[:,pca],axis=1)))+\",\")\n",
    "    for i in range(clusterGMMCovResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(clusterGMMCovResultsFinetune[:,i,pca]), np.std(np.mean(clusterGMMCovResultsFinetune[:,i,pca],axis=1))))\n",
    "    \n",
    "    f.write(\"\\n**** correlation with size (smaller is better) ****\")\n",
    "    f.write(\"\\nwithout PCA pyradiomics\")\n",
    "    pca = 0\n",
    "    f.write(\"\\n %.4f(%.5f) \" % (np.mean(corrResults[:, pca]), np.std(corrResults[:, pca]))+\",\")\n",
    "    for i in range(corrResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(corrResultsFinetune[:,i,pca]), np.std(corrResultsFinetune[:,i,pca])))\n",
    "    \n",
    "    f.write(\"\\nwith PCA pyradiomics \")\n",
    "    pca = 1\n",
    "    f.write(\"\\n %.4f(%.5f) \" % (np.mean(corrResults[:, pca]), np.std(corrResults[:,pca]))+\",\")\n",
    "    for i in range(corrResultsFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(corrResultsFinetune[:,i,pca]), np.std(corrResultsFinetune[:,i,pca])))\n",
    "    \n",
    "    f.write(\"\\n**** Accuracy during finetuning: \")\n",
    "    for i in range(accuracyFinetune.shape[1]):\n",
    "        f.write(\"%.4f(%.5f) \" % (np.mean(accuracyFinetune[:,i]), np.std(accuracyFinetune[:,i])))\n",
    "         \n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runShallowTrain(featPy):\n",
    "    # Average the features within cartridge\n",
    "    featuresAv = cartridgeAverage(featPy,array_scan,array_labels)\n",
    "    shapeFeat = featuresAv.shape\n",
    "    labelsAv = np.zeros(shapeFeat[0]*shapeFeat[1],dtype=np.int)\n",
    "    scansAv = np.zeros(shapeFeat[0]*shapeFeat[1],dtype=np.int)\n",
    "    i=0\n",
    "    for i0 in range(shapeFeat[0]):\n",
    "        for i1 in range(shapeFeat[1]):\n",
    "            labelsAv[i]=i1\n",
    "            scansAv[i]=i0\n",
    "            i+=1\n",
    "    featuresAv = np.reshape(featuresAv,(shapeFeat[0]*shapeFeat[1],shapeFeat[2]))\n",
    "\n",
    "    # Then normalize \n",
    "    av = np.mean(featuresAv,axis=0)\n",
    "    stdv = np.std(featuresAv,axis=0)\n",
    "    featuresNorm = (featuresAv-av)/stdv\n",
    "    # Remove NaN\n",
    "    featuresNorm = featuresNorm[:,~np.isnan(featuresNorm).any(axis=0)]\n",
    "\n",
    "    # Normalize all features for training the network:\n",
    "    av = np.mean(featPy,axis=0)\n",
    "    stdv = np.std(featPy,axis=0)\n",
    "    featPy = (featPy-av)/stdv\n",
    "    # Remove NaN\n",
    "    featPy = featPy[:,~np.isnan(featPy).any(axis=0)]\n",
    "\n",
    "    ## Do PCA on all data:\n",
    "    featuresPCA = decomposition.PCA(n_components = nPCA).fit_transform(featuresNorm) #HERE\n",
    "\n",
    "    nbFeat = featuresAv.shape[1]\n",
    "\n",
    "    # If we train on all scan/all labels, we test also on all scan/all labels as there is no test left.\n",
    "    nTestLabels = nbClass\n",
    "    nTestScans = nbScans\n",
    "    if not nTrainLabels==10:\n",
    "        nTestLabels = nbClass-nTrainLabels\n",
    "    if not nTrainScans==17:\n",
    "        nTestScans = nbScans-nTrainScans\n",
    "\n",
    "    y_all=list()\n",
    "    for i in range(featPy.shape[0]):\n",
    "        y_all.append(array_labels[i])\n",
    "    y_all=np.asarray(y_all)\n",
    "\n",
    "    y_allAv=list()\n",
    "    for i in range(featuresNorm.shape[0]):\n",
    "        y_allAv.append(labelsAv[i])\n",
    "    y_allAv=np.asarray(y_allAv)\n",
    "\n",
    "    clusterResults = np.zeros((nbRun,3,3)) # 3: without PCA, with PCA, with LDA, 3: pearson homogeneity, completeness and v_measure\n",
    "    clusterGMMResults = np.zeros((nbRun,3,3))\n",
    "    clusterGMMCovResults = np.zeros((nbRun,3,nTestLabels))\n",
    "    corrResults = np.zeros((nbRun,3)) # 3: with and without pca, and LDA\n",
    "    iccAvResults = np.zeros((nbRun,3)) # 3: with and without pca, and LDA\n",
    "\n",
    "    clusterResultsFinetune = np.zeros((nbRun,totalEpochs+1,2,3)) # 2: with and without PCA, 3: pearson homogeneity, completeness and v_measure\n",
    "    clusterGMMResultsFinetune = np.zeros((nbRun,totalEpochs+1,2,3))\n",
    "    clusterGMMCovResultsFinetune = np.zeros((nbRun,totalEpochs+1,2,nTestLabels))\n",
    "    corrResultsFinetune = np.zeros((nbRun,totalEpochs+1,2)) # 2: with and without pca\n",
    "    iccAvResultsFinetune = np.zeros((nbRun,totalEpochs+1,2)) # 2: with and without pca\n",
    "    accuracyFinetune = np.zeros((nbRun,totalEpochs))\n",
    "\n",
    "    listLabels = range(nbClass)\n",
    "    listScans = range(nbScans)\n",
    "    for iRun in range(nbRun):\n",
    "\n",
    "        x_train=list()\n",
    "        y_train=list()\n",
    "\n",
    "        np.random.seed(iRun) # for reproducibility\n",
    "        np.random.shuffle(listLabels)\n",
    "        trainLabels=sorted(listLabels[0:nTrainLabels])\n",
    "        np.random.shuffle(listScans)\n",
    "        trainScans=sorted(listScans[0:nTrainScans])\n",
    "        print iRun,'***************************** training labels:',trainLabels,'scans',trainScans,'*****************************'\n",
    "        # First do the train mask for finetuning:\n",
    "        mask = np.zeros(y_all.shape, dtype=bool)\n",
    "        mask2 = np.zeros(y_all.shape, dtype=bool)\n",
    "        for n in trainLabels:\n",
    "            mask |= (y_all==n)\n",
    "        for n in trainScans:\n",
    "            mask2 |= (array_scan==n)\n",
    "        maskTrain = mask & mask2\n",
    "        x_train = featPy[maskTrain]\n",
    "        y_train = y_all[maskTrain]\n",
    "        # Minimize the number of output labels (0001112222... instead of 0003334444...):\n",
    "        for i in range(len(np.unique(y_train))):\n",
    "            y_train[y_train==np.unique(y_train)[i]]=i\n",
    "        # Convert labels to categorical one-hot encoding\n",
    "        y_train = to_categorical(y_train, num_classes=nTrainLabels)\n",
    "\n",
    "        # Then the test mask:\n",
    "        maskLabels = []\n",
    "        maskScans = []        \n",
    "\n",
    "        if nTrainLabels==10:\n",
    "            lL=[]\n",
    "        else:\n",
    "            lL=trainLabels\n",
    "        if nTrainScans==17:\n",
    "            lS=[]\n",
    "        else:\n",
    "            lS=trainScans\n",
    "\n",
    "        maskTest = np.ones(array_labels.shape, dtype=bool)\n",
    "        for n in lL:\n",
    "            maskTest &= (array_labels!=n)\n",
    "        for n in lS:\n",
    "            maskTest &= (array_scan!=n)\n",
    "\n",
    "        # Now do the same for the clustering of the averaged and normalized Pyradiomics features:\n",
    "        mask = np.zeros(y_allAv.shape, dtype=bool)\n",
    "        mask2 = np.zeros(y_allAv.shape, dtype=bool)\n",
    "        for n in trainLabels:\n",
    "            mask |= (y_allAv==n)\n",
    "        for n in trainScans:\n",
    "            mask2 |= (scansAv==n)\n",
    "        maskTrainAv = mask & mask2\n",
    "        # Then the test mask:\n",
    "        maskLabels = []\n",
    "        maskScans = []        \n",
    "        if nTrainLabels==10:\n",
    "            lL=[]\n",
    "        else:\n",
    "            lL=trainLabels\n",
    "        if nTrainScans==17:\n",
    "            lS=[]\n",
    "        else:\n",
    "            lS=trainScans\n",
    "        maskTestAv = np.ones(labelsAv.shape, dtype=bool)\n",
    "        for n in lL:\n",
    "            maskTestAv &= (labelsAv!=n)\n",
    "        for n in lS:\n",
    "            maskTestAv &= (scansAv!=n)\n",
    "\n",
    "        # do LDA\n",
    "        lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=nPCA) # same number of components as the PCA\n",
    "        featuresLDA = lda.fit(featuresNorm[maskTrainAv], labelsAv[maskTrainAv]).transform(featuresNorm)\n",
    "        av = np.mean(featuresLDA,axis=0)\n",
    "        stdv = np.std(featuresLDA,axis=0)\n",
    "        featuresLDA = (featuresLDA-av)/stdv\n",
    "\n",
    "        ##Intraclass Correlation Coefficient (ICC)\n",
    "        print \"ICC\"\n",
    "        iccAvResults[iRun,0] = iccAverage(featuresNorm[maskTestAv],nTestLabels,nTestScans,type_icc)\n",
    "        print \"ICC PCA\"\n",
    "        iccAvResults[iRun,1] = iccAverage(featuresPCA[maskTestAv],nTestLabels,nTestScans,type_icc)\n",
    "        print \"ICC LDA\"\n",
    "        iccAvResults[iRun,2] = iccAverage(featuresLDA[maskTestAv],nTestLabels,nTestScans,type_icc)\n",
    "\n",
    "        ## KMEANS clustering\n",
    "        print \"KMEANS cluster\"\n",
    "        clusterResults[iRun,0,:] = clusterFeatures(featuresNorm[maskTestAv],labelsAv[maskTestAv], norm = \"normal\")\n",
    "        print \"KMEANS cluster PCA\"\n",
    "        clusterResults[iRun,1,:] = clusterFeatures(featuresPCA[maskTestAv],labelsAv[maskTestAv], norm = \"normal\")\n",
    "        print \"KMEANS cluster LDA\"\n",
    "        clusterResults[iRun,2,:] = clusterFeatures(featuresLDA[maskTestAv],labelsAv[maskTestAv], norm = \"normal\")\n",
    "\n",
    "        # GMM clustering\n",
    "        print \"GMM cluster\"\n",
    "        clusterGMMResults[iRun,0,:],clusterGMMCovResults[iRun,0,:] = clusterGMMFeatures(featuresNorm[maskTestAv],labelsAv[maskTestAv], norm = \"normal\")\n",
    "        print \"GMM cluster PCA\"\n",
    "        clusterGMMResults[iRun,1,:],clusterGMMCovResults[iRun,1,:] = clusterGMMFeatures(featuresPCA[maskTestAv],labelsAv[maskTestAv], norm = \"normal\")\n",
    "        print \"GMM cluster LDA\"\n",
    "        clusterGMMResults[iRun,2,:],clusterGMMCovResults[iRun,2,:] = clusterGMMFeatures(featuresLDA[maskTestAv],labelsAv[maskTestAv], norm = \"normal\")\n",
    "\n",
    "\n",
    "        # Correlate with size before finetuning\n",
    "        print \"correlate size\"\n",
    "        corrResults[iRun,0] = correlateSize(featuresNorm[maskTestAv],scansAv[maskTestAv],labelsAv[maskTestAv])\n",
    "        print \"correlate size PCA\"\n",
    "        corrResults[iRun,1] = correlateSize(featuresPCA[maskTestAv],scansAv[maskTestAv],labelsAv[maskTestAv])\n",
    "        print \"correlate size LDA\"\n",
    "        corrResults[iRun,2] = correlateSize(featuresLDA[maskTestAv],scansAv[maskTestAv],labelsAv[maskTestAv])\n",
    "\n",
    "\n",
    "        K.clear_session()\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        # Sequential:\n",
    "        model = Sequential([\n",
    "            Dense(nNeurons, kernel_initializer=initializer,bias_initializer='zeros',input_shape=(x_train.shape[1],)),\n",
    "            Dropout(0.5), # dropout before ReLU same results: https://sebastianraschka.com/faq/docs/dropout-activation.html\n",
    "            Activation('relu'), Dense(nTrainLabels), Activation('softmax'),])\n",
    "        \n",
    "        l_r = 0.0001\n",
    "        decay_rate = 0\n",
    "        adam = optimizers.Adam(lr=l_r, beta_1=0.9, beta_2=0.999, epsilon=1e-08,decay=decay_rate)\n",
    "        sgd = optimizers.SGD(lr=l_r, momentum=0.9)\n",
    "        \n",
    "        model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "        # Before training, test the features with random weights:\n",
    "        intermediate_layer_model = Model(inputs=model.input, outputs=[model.get_layer('activation_1').output])\n",
    "        featuresShallow = intermediate_layer_model.predict(featPy[maskTest])\n",
    "\n",
    "        # Average the features within cartridge:\n",
    "        featuresShallow = cartridgeAverage(featuresShallow,array_scan[maskTest],array_labels[maskTest])\n",
    "        shapeFeat = featuresShallow.shape\n",
    "        featuresShallow = np.reshape(featuresShallow,(shapeFeat[0]*shapeFeat[1],shapeFeat[2]))\n",
    "\n",
    "        # Normalize the features (the results are better with this normalization):\n",
    "        av = np.mean(featuresShallow,axis=0)\n",
    "        stdv = np.std(featuresShallow,axis=0)\n",
    "        featuresShallow = (featuresShallow-av)/stdv\n",
    "\n",
    "        # Remove NaN\n",
    "        featuresShallow = featuresShallow[:,~np.isnan(featuresShallow).any(axis=0)]\n",
    "\n",
    "        # do PCA\n",
    "        featuresShallowPCA = decomposition.PCA(n_components = nPCA).fit_transform(featuresShallow)   \n",
    "\n",
    "        #Intraclass Correlation Coefficient (ICC)\n",
    "        print \"ICC finetune\"\n",
    "        iccAvResultsFinetune[iRun,0,0] = iccAverage(featuresShallow,nTestLabels,nTestScans,type_icc)\n",
    "        iccAvResultsFinetune[iRun,0,1] = iccAverage(featuresShallowPCA,nTestLabels,nTestScans,type_icc)\n",
    "\n",
    "\n",
    "        ## KMEANS clustering\n",
    "        print \"KMEANS cluster\"\n",
    "        clusterResultsFinetune[iRun,0,0,:] = clusterFeatures(featuresShallow,labelsAv[maskTestAv], norm = \"normal\")\n",
    "        clusterResultsFinetune[iRun,0,1,:] = clusterFeatures(featuresShallowPCA,labelsAv[maskTestAv], norm = \"normal\")\n",
    "\n",
    "        # GMM clustering\n",
    "        print \"GMM cluster\"\n",
    "        clusterGMMResultsFinetune[iRun,0,0,:],clusterGMMCovResultsFinetune[iRun,0,0,:] = clusterGMMFeatures(featuresShallow,labelsAv[maskTestAv], norm = \"normal\")\n",
    "        clusterGMMResultsFinetune[iRun,0,1,:],clusterGMMCovResultsFinetune[iRun,0,1,:] = clusterGMMFeatures(featuresShallowPCA,labelsAv[maskTestAv], norm = \"normal\")\n",
    "\n",
    "        # Correlate with size\n",
    "        print \"correlate size\"\n",
    "        corrResultsFinetune[iRun,0,0] = correlateSize(featuresShallow,scansAv[maskTestAv],labelsAv[maskTestAv])\n",
    "        corrResultsFinetune[iRun,0,1] = correlateSize(featuresShallowPCA,scansAv[maskTestAv],labelsAv[maskTestAv])\n",
    "\n",
    "        for iEp in range(totalEpochs): \n",
    "            hist = model.fit(x_train, y_train, epochs=10, batch_size=32)# epochs=10. In the paper I wrote 50 epochs but it was actually 500. Check if 50 makes a difference.\n",
    "            \n",
    "            intermediate_layer_model = Model(inputs=model.input, outputs=[model.get_layer('activation_1').output])\n",
    "            featuresShallow = intermediate_layer_model.predict(featPy[maskTest])\n",
    "\n",
    "            # Average the features within cartridge:\n",
    "            featuresShallow = cartridgeAverage(featuresShallow,array_scan[maskTest],array_labels[maskTest])\n",
    "            shapeFeat = featuresShallow.shape\n",
    "            featuresShallow = np.reshape(featuresShallow,(shapeFeat[0]*shapeFeat[1],shapeFeat[2]))\n",
    "\n",
    "            # Normalize the features (the results are better with this normalization):\n",
    "            av = np.mean(featuresShallow,axis=0)\n",
    "            stdv = np.std(featuresShallow,axis=0)\n",
    "            featuresShallow = (featuresShallow-av)/stdv\n",
    "\n",
    "            # Remove NaN\n",
    "            featuresShallow = featuresShallow[:,~np.isnan(featuresShallow).any(axis=0)]\n",
    "\n",
    "            # do PCA\n",
    "            featuresShallowPCA = decomposition.PCA(n_components = nPCA).fit_transform(featuresShallow)   \n",
    "        \n",
    "            # do LDA\n",
    "\n",
    "            # Accuracy:\n",
    "            accuracyFinetune[iRun,iEp] = hist.history['acc'][0]\n",
    "\n",
    "            #Intraclass Correlation Coefficient (ICC)\n",
    "            print \"ICC finetune\"\n",
    "            iccAvResultsFinetune[iRun,iEp+1,0] = iccAverage(featuresShallow,nTestLabels,nTestScans,type_icc)\n",
    "            iccAvResultsFinetune[iRun,iEp+1,1] = iccAverage(featuresShallowPCA,nTestLabels,nTestScans,type_icc)\n",
    "\n",
    "            ## KMEANS clustering\n",
    "            print \"KMEANS cluster\"\n",
    "            clusterResultsFinetune[iRun,iEp+1,0,:] = clusterFeatures(featuresShallow,labelsAv[maskTestAv], norm = \"normal\")\n",
    "            clusterResultsFinetune[iRun,iEp+1,1,:] = clusterFeatures(featuresShallowPCA,labelsAv[maskTestAv], norm = \"normal\")\n",
    "\n",
    "            # GMM clustering\n",
    "            print \"GMM cluster\"\n",
    "            clusterGMMResultsFinetune[iRun,iEp+1,0,:],clusterGMMCovResultsFinetune[iRun,iEp+1,0,:] = clusterGMMFeatures(featuresShallow,labelsAv[maskTestAv], norm = \"normal\")\n",
    "            clusterGMMResultsFinetune[iRun,iEp+1,1,:],clusterGMMCovResultsFinetune[iRun,iEp+1,1,:] = clusterGMMFeatures(featuresShallowPCA,labelsAv[maskTestAv], norm = \"normal\")\n",
    "\n",
    "            # Correlate with size\n",
    "            print \"correlate size\"\n",
    "            corrResultsFinetune[iRun,iEp+1,0] = correlateSize(featuresShallow,scansAv[maskTestAv],labelsAv[maskTestAv])\n",
    "            corrResultsFinetune[iRun,iEp+1,1] = correlateSize(featuresShallowPCA,scansAv[maskTestAv],labelsAv[maskTestAv])\n",
    "            \n",
    "        listResults = [iccAvResultsFinetune,iccAvResults,clusterResultsFinetune,clusterResults,\n",
    "                       clusterGMMResultsFinetune,clusterGMMResults,clusterGMMCovResultsFinetune,\n",
    "                       clusterGMMCovResults,corrResultsFinetune,corrResults,accuracyFinetune]\n",
    "        writeResults(listResults,path_results)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nbClass = 10\n",
    "nbScans = 17\n",
    "nTrainLabels = 5\n",
    "nTrainScans = 17\n",
    "nbRun = 100\n",
    "totalEpochs = 50\n",
    "type_icc = 'icc2'\n",
    "\n",
    "\n",
    "nNeurons = 100\n",
    "nPCA = 4\n",
    "nPCA = min(nPCA,nNeurons)\n",
    "initializer = 'random_uniform'\n",
    "resultName = 'Shallow_lab50x1'+str(nTrainLabels)+'_Sc'+str(nTrainScans)+'_nrun'+str(nbRun)+'_nPCA'+str(nPCA)+'_nNeurons'+str(nNeurons)+'_'+initializer\n",
    "print 'resultName:',resultName\n",
    "if not os.path.exists('./results/'+resultName):\n",
    "    os.makedirs('./results/'+resultName)\n",
    "path_results = './results/'+resultName+'/'\n",
    "runShallowTrain(featuresPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
